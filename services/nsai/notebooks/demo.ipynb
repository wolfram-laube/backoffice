{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§  NSAI - Neurosymbolic AI Runner Selection\n",
    "\n",
    "> Interactive Demo & Playground for the Symbolic Layer\n",
    "\n",
    "**ADR:** AI-001 | **Epic:** #27 | **Paper:** #26 (JKU Bachelor)\n",
    "\n",
    "---\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  SYMBOLIC LAYER (This Notebook)                     â”‚\n",
    "â”‚  â”œâ”€â”€ ontology/   Runner Capability Ontology         â”‚\n",
    "â”‚  â”œâ”€â”€ parser/     Job Requirement Parser             â”‚\n",
    "â”‚  â””â”€â”€ csp/        Constraint Satisfaction Solver     â”‚\n",
    "â”‚                       â†“                             â”‚\n",
    "â”‚              [Feasible Runner Set]                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  SUBSYMBOLIC LAYER (runner_bandit service)          â”‚\n",
    "â”‚  â””â”€â”€ Multi-Armed Bandit Selection (UCB1, Thompson)  â”‚\n",
    "â”‚                       â†“                             â”‚\n",
    "â”‚              [Selected Runner]                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”§ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab Setup (uncomment if running in Colab)\n",
    "# !pip install pyyaml networkx matplotlib --quiet\n",
    "# !git clone https://github.com/blauweiss-llc/backoffice.git /content/backoffice\n",
    "# import sys; sys.path.insert(0, '/content/backoffice/services')\n",
    "\n",
    "# Local Setup\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent to path for imports\n",
    "notebook_dir = Path('.').absolute()\n",
    "if 'services/nsai' in str(notebook_dir):\n",
    "    sys.path.insert(0, str(notebook_dir.parent.parent))\n",
    "else:\n",
    "    sys.path.insert(0, str(notebook_dir.parent))\n",
    "\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"Working dir: {notebook_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from nsai.ontology import RunnerOntology\n",
    "from nsai.ontology.runner_ontology import (\n",
    "    Runner, RunnerCapability, CapabilityType, create_blauweiss_ontology\n",
    ")\n",
    "from nsai.parser import JobRequirementParser, JobRequirements\n",
    "from nsai.csp import ConstraintSolver\n",
    "from nsai.csp.constraint_solver import SolverStatus\n",
    "\n",
    "# Visualization\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "print(\"âœ… NSAI modules loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1ï¸âƒ£ Ontology - Runner Capabilities\n",
    "\n",
    "The ontology defines **what runners can do** using a capability taxonomy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh ontology\n",
    "onto = RunnerOntology()\n",
    "\n",
    "# Show built-in capability types\n",
    "print(\"ğŸ“š Capability Types:\")\n",
    "for cap_type in CapabilityType:\n",
    "    print(f\"  â€¢ {cap_type.name}\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Standard Capabilities: {len(onto.STANDARD_CAPABILITIES)}\")\n",
    "for name, ctype in list(onto.STANDARD_CAPABILITIES.items())[:10]:\n",
    "    print(f\"  â€¢ {name:15} â†’ {ctype.name}\")\n",
    "print(\"  ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add runners with capabilities\n",
    "onto.add_runner(\n",
    "    name=\"gitlab-runner-nordic\",\n",
    "    runner_id=12345,\n",
    "    capabilities=[\"docker\", \"shell\", \"gcp\", \"nordic\", \"linux\"],\n",
    "    tags=[\"docker-any\", \"shell\", \"nordic\"],\n",
    "    cost_per_minute=0.01\n",
    ")\n",
    "\n",
    "onto.add_runner(\n",
    "    name=\"gpu-runner-aws\",\n",
    "    capabilities=[\"docker\", \"gpu\", \"aws\", \"linux\"],\n",
    "    tags=[\"docker\", \"gpu\"],\n",
    "    cost_per_minute=0.50  # GPUs are expensive!\n",
    ")\n",
    "\n",
    "onto.add_runner(\n",
    "    name=\"mac-local\",\n",
    "    capabilities=[\"shell\", \"macos\"],\n",
    "    tags=[\"shell\", \"macos\"],\n",
    "    cost_per_minute=0.0,\n",
    "    online=False  # Currently offline\n",
    ")\n",
    "\n",
    "onto.add_runner(\n",
    "    name=\"k8s-cluster\",\n",
    "    capabilities=[\"kubernetes\", \"docker\", \"linux\", \"azure\"],\n",
    "    tags=[\"k8s\", \"azure\"],\n",
    "    cost_per_minute=0.05\n",
    ")\n",
    "\n",
    "print(f\"âœ… Added {len(onto.runners)} runners\")\n",
    "for name, runner in onto.runners.items():\n",
    "    caps = [c.name for c in runner.capabilities]\n",
    "    status = \"ğŸŸ¢\" if runner.online else \"ğŸ”´\"\n",
    "    print(f\"  {status} {name}: {caps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the ontology\n",
    "print(\"ğŸ” Runners with 'docker' capability:\")\n",
    "for r in onto.get_runners_with_capability(\"docker\"):\n",
    "    print(f\"  â€¢ {r.name}\")\n",
    "\n",
    "print(\"\\nğŸ” Runners with 'docker' AND 'linux':\")\n",
    "for r in onto.get_runners_with_all_capabilities([\"docker\", \"linux\"]):\n",
    "    print(f\"  â€¢ {r.name}\")\n",
    "\n",
    "print(\"\\nğŸ” Feasible runners (require docker, exclude gpu):\")\n",
    "for r in onto.get_feasible_runners(required=[\"docker\"], excluded=[\"gpu\"]):\n",
    "    print(f\"  â€¢ {r.name} (cost: â‚¬{r.cost_per_minute}/min)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¨ Capability Implications\n",
    "\n",
    "Some capabilities **imply** others (e.g., `nordic` â†’ `gcp`, `eu-west`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ”— Capability Implications:\")\n",
    "for cap, implied in onto.CAPABILITY_IMPLICATIONS.items():\n",
    "    print(f\"  {cap} â†’ {implied}\")\n",
    "\n",
    "# Check nordic runner - should have implied capabilities\n",
    "nordic = onto.runners[\"gitlab-runner-nordic\"]\n",
    "print(f\"\\nğŸ“‹ Nordic runner capabilities (with implications):\")\n",
    "for cap in sorted(nordic.capabilities, key=lambda c: c.name):\n",
    "    print(f\"  â€¢ {cap.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2ï¸âƒ£ Parser - Job Requirements\n",
    "\n",
    "The parser extracts **what jobs need** from `.gitlab-ci.yml` definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = JobRequirementParser()\n",
    "\n",
    "# Simple job with tags\n",
    "job1 = {\n",
    "    \"tags\": [\"docker-any\"],\n",
    "    \"image\": \"python:3.11\",\n",
    "    \"script\": [\"pytest\"]\n",
    "}\n",
    "\n",
    "reqs1 = parser.parse(job1, \"unit-tests\")\n",
    "print(\"ğŸ“‹ Job: unit-tests\")\n",
    "print(f\"  Required:  {reqs1.required_capabilities}\")\n",
    "print(f\"  Preferred: {reqs1.preferred_capabilities}\")\n",
    "print(f\"  Tags:      {reqs1.tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU job with NVIDIA image\n",
    "job2 = {\n",
    "    \"tags\": [\"docker\", \"gpu\"],\n",
    "    \"image\": \"nvidia/cuda:11.8-base\",\n",
    "    \"timeout\": \"2h 30m\",\n",
    "    \"script\": [\"python train.py\"]\n",
    "}\n",
    "\n",
    "reqs2 = parser.parse(job2, \"ml-training\")\n",
    "print(\"ğŸ“‹ Job: ml-training\")\n",
    "print(f\"  Required:  {reqs2.required_capabilities}\")\n",
    "print(f\"  Preferred: {reqs2.preferred_capabilities}\")\n",
    "print(f\"  Timeout:   {reqs2.timeout_seconds}s ({reqs2.timeout_seconds/60:.0f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse complete .gitlab-ci.yml\n",
    "yaml_content = \"\"\"\n",
    "default:\n",
    "  tags:\n",
    "    - docker-any\n",
    "\n",
    "stages:\n",
    "  - test\n",
    "  - build\n",
    "  - deploy\n",
    "\n",
    "unit-tests:\n",
    "  stage: test\n",
    "  image: python:3.11\n",
    "  script:\n",
    "    - pytest tests/\n",
    "\n",
    "build-image:\n",
    "  stage: build\n",
    "  tags:\n",
    "    - docker-any\n",
    "    - gcp\n",
    "  services:\n",
    "    - docker:dind\n",
    "  script:\n",
    "    - docker build -t myapp .\n",
    "\n",
    "deploy-k8s:\n",
    "  stage: deploy\n",
    "  tags:\n",
    "    - k8s\n",
    "  script:\n",
    "    - kubectl apply -f k8s/\n",
    "\"\"\"\n",
    "\n",
    "jobs = parser.parse_yaml(yaml_content)\n",
    "print(f\"ğŸ“‹ Parsed {len(jobs)} jobs from YAML:\\n\")\n",
    "for name, reqs in jobs.items():\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    Required: {reqs.required_capabilities}\")\n",
    "    if reqs.preferred_capabilities:\n",
    "        print(f\"    Preferred: {reqs.preferred_capabilities}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3ï¸âƒ£ CSP Solver - Finding Feasible Runners\n",
    "\n",
    "The solver combines ontology + requirements to find **which runners can execute a job**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create solver with our ontology\n",
    "solver = ConstraintSolver(ontology=onto, parser=parser)\n",
    "\n",
    "# Solve for a simple Docker job\n",
    "result = solver.solve({\"tags\": [\"docker-any\"]}, \"simple-job\")\n",
    "\n",
    "print(f\"ğŸ¯ Result for 'simple-job':\")\n",
    "print(f\"  Status: {result.status.name}\")\n",
    "print(f\"  Feasible: {result.feasible_runners}\")\n",
    "print(f\"  Best: {result.best_runner}\")\n",
    "print(f\"  Solve time: {result.solve_time_ms:.2f}ms\")\n",
    "print(f\"\\nğŸ“ Explanation:\\n{result.explanation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve for GPU job - should prefer gpu-runner\n",
    "result_gpu = solver.solve(\n",
    "    {\"tags\": [\"docker\"], \"image\": \"nvidia/cuda:11.8\"},\n",
    "    \"gpu-training\"\n",
    ")\n",
    "\n",
    "print(f\"ğŸ¯ Result for 'gpu-training':\")\n",
    "print(f\"  Ranked runners:\")\n",
    "for name, score in result_gpu.ranked_runners:\n",
    "    runner = onto.runners[name]\n",
    "    print(f\"    {name}: score={score:.2f}, cost=â‚¬{runner.cost_per_minute}/min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infeasible job - no runner can do this\n",
    "result_impossible = solver.solve(\n",
    "    {\"tags\": [\"quantum-computer\", \"docker\"]},  # We don't have a quantum computer!\n",
    "    \"quantum-job\"\n",
    ")\n",
    "\n",
    "print(f\"ğŸ¯ Result for 'quantum-job':\")\n",
    "print(f\"  Status: {result_impossible.status.name}\")\n",
    "print(f\"  Feasible: {result_impossible.feasible_runners}\")\n",
    "print(f\"\\nğŸ“ Explanation:\\n{result_impossible.explanation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch solve for all jobs in a pipeline\n",
    "pipeline_jobs = {\n",
    "    \"lint\": {\"tags\": [\"docker-any\"], \"image\": \"python:3.11\"},\n",
    "    \"test\": {\"tags\": [\"docker-any\"], \"image\": \"python:3.11\"},\n",
    "    \"build\": {\"tags\": [\"docker-any\", \"gcp\"]},\n",
    "    \"deploy\": {\"tags\": [\"k8s\"]},\n",
    "    \"train-ml\": {\"tags\": [\"docker\", \"gpu\"], \"image\": \"nvidia/cuda:11.8\"}\n",
    "}\n",
    "\n",
    "results = solver.solve_batch(pipeline_jobs)\n",
    "\n",
    "print(\"ğŸ“‹ Pipeline Feasibility Matrix:\\n\")\n",
    "print(f\"{'Job':<15} {'Status':<12} {'Best Runner':<25} {'Alternatives'}\")\n",
    "print(\"-\" * 70)\n",
    "for job_name, result in results.items():\n",
    "    status = \"âœ…\" if result.is_feasible else \"âŒ\"\n",
    "    best = result.best_runner or \"NONE\"\n",
    "    alts = len(result.feasible_runners) - 1 if result.feasible_runners else 0\n",
    "    print(f\"{job_name:<15} {status:<12} {best:<25} +{alts} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Visualization\n",
    "\n",
    "Let's visualize the runner-capability relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Install visualization libraries\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import networkx as nx\n",
    "    VIZ_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Install matplotlib and networkx for visualizations:\")\n",
    "    print(\"   pip install matplotlib networkx\")\n",
    "    VIZ_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VIZ_AVAILABLE:\n",
    "    # Create capability graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add runner nodes\n",
    "    for name, runner in onto.runners.items():\n",
    "        color = \"lightgreen\" if runner.online else \"lightcoral\"\n",
    "        G.add_node(name, node_type=\"runner\", color=color)\n",
    "    \n",
    "    # Add capability nodes and edges\n",
    "    all_caps = set()\n",
    "    for runner in onto.runners.values():\n",
    "        for cap in runner.capabilities:\n",
    "            all_caps.add(cap.name)\n",
    "            G.add_node(cap.name, node_type=\"capability\", color=\"lightblue\")\n",
    "            G.add_edge(runner.name, cap.name)\n",
    "    \n",
    "    # Draw\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
    "    \n",
    "    # Draw runners\n",
    "    runner_nodes = [n for n, d in G.nodes(data=True) if d.get('node_type') == 'runner']\n",
    "    runner_colors = [G.nodes[n]['color'] for n in runner_nodes]\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=runner_nodes, node_color=runner_colors,\n",
    "                           node_size=2000, node_shape='s')\n",
    "    \n",
    "    # Draw capabilities\n",
    "    cap_nodes = [n for n, d in G.nodes(data=True) if d.get('node_type') == 'capability']\n",
    "    nx.draw_networkx_nodes(G, pos, nodelist=cap_nodes, node_color='lightblue',\n",
    "                           node_size=1000, node_shape='o')\n",
    "    \n",
    "    # Draw edges and labels\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "    \n",
    "    plt.title(\"ğŸ§  NSAI Runner-Capability Graph\\n(ğŸŸ© = online, ğŸŸ¥ = offline, ğŸ”µ = capability)\")\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Skipping visualization (libraries not installed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VIZ_AVAILABLE:\n",
    "    # Feasibility heatmap\n",
    "    import numpy as np\n",
    "    \n",
    "    # Jobs to test\n",
    "    test_jobs = {\n",
    "        \"docker-only\": {\"tags\": [\"docker-any\"]},\n",
    "        \"docker+gcp\": {\"tags\": [\"docker-any\", \"gcp\"]},\n",
    "        \"gpu-job\": {\"tags\": [\"docker\", \"gpu\"]},\n",
    "        \"k8s-deploy\": {\"tags\": [\"k8s\"]},\n",
    "        \"macos-build\": {\"tags\": [\"shell\", \"macos\"]},\n",
    "        \"azure-k8s\": {\"tags\": [\"kubernetes\", \"azure\"]},\n",
    "    }\n",
    "    \n",
    "    runner_names = list(onto.runners.keys())\n",
    "    job_names = list(test_jobs.keys())\n",
    "    \n",
    "    # Build matrix\n",
    "    matrix = np.zeros((len(job_names), len(runner_names)))\n",
    "    for i, (job_name, job_def) in enumerate(test_jobs.items()):\n",
    "        result = solver.solve(job_def, job_name, include_offline=True)\n",
    "        for j, runner_name in enumerate(runner_names):\n",
    "            if runner_name in result.feasible_runners:\n",
    "                # Score based on ranking\n",
    "                for rn, score in result.ranked_runners:\n",
    "                    if rn == runner_name:\n",
    "                        matrix[i, j] = score\n",
    "                        break\n",
    "    \n",
    "    # Plot heatmap\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    im = ax.imshow(matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "    \n",
    "    ax.set_xticks(range(len(runner_names)))\n",
    "    ax.set_xticklabels(runner_names, rotation=45, ha='right')\n",
    "    ax.set_yticks(range(len(job_names)))\n",
    "    ax.set_yticklabels(job_names)\n",
    "    \n",
    "    # Add values\n",
    "    for i in range(len(job_names)):\n",
    "        for j in range(len(runner_names)):\n",
    "            val = matrix[i, j]\n",
    "            color = 'white' if val > 0.5 else 'black'\n",
    "            text = f\"{val:.1f}\" if val > 0 else \"âœ—\"\n",
    "            ax.text(j, i, text, ha='center', va='center', color=color, fontsize=10)\n",
    "    \n",
    "    plt.colorbar(im, label='Preference Score')\n",
    "    plt.title('ğŸ¯ Job-Runner Feasibility Matrix\\n(0 = infeasible, 1 = perfect match)')\n",
    "    plt.xlabel('Runners')\n",
    "    plt.ylabel('Jobs')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5ï¸âƒ£ Experiments (Paper Material)\n",
    "\n",
    "Benchmarks and comparisons for the JKU Bachelor thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def benchmark_solver(num_runners: int, num_jobs: int, num_caps: int = 5) -> dict:\n",
    "    \"\"\"Benchmark CSP solver with synthetic data.\"\"\"\n",
    "    # Create ontology with random runners\n",
    "    onto = RunnerOntology()\n",
    "    all_caps = [\"docker\", \"shell\", \"linux\", \"macos\", \"windows\", \n",
    "                \"gcp\", \"aws\", \"azure\", \"gpu\", \"arm64\", \"kubernetes\"]\n",
    "    \n",
    "    for i in range(num_runners):\n",
    "        caps = random.sample(all_caps, min(num_caps, len(all_caps)))\n",
    "        onto.add_runner(f\"runner-{i}\", capabilities=caps)\n",
    "    \n",
    "    # Create solver\n",
    "    solver = ConstraintSolver(onto, JobRequirementParser())\n",
    "    \n",
    "    # Generate random jobs\n",
    "    jobs = {}\n",
    "    for i in range(num_jobs):\n",
    "        num_required = random.randint(1, 3)\n",
    "        tags = random.sample(all_caps, num_required)\n",
    "        jobs[f\"job-{i}\"] = {\"tags\": tags}\n",
    "    \n",
    "    # Benchmark\n",
    "    start = time.perf_counter()\n",
    "    results = solver.solve_batch(jobs)\n",
    "    elapsed = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    # Stats\n",
    "    feasible = sum(1 for r in results.values() if r.is_feasible)\n",
    "    \n",
    "    return {\n",
    "        \"runners\": num_runners,\n",
    "        \"jobs\": num_jobs,\n",
    "        \"total_ms\": elapsed,\n",
    "        \"per_job_ms\": elapsed / num_jobs,\n",
    "        \"feasible_rate\": feasible / num_jobs\n",
    "    }\n",
    "\n",
    "print(\"ğŸ”¬ Running benchmarks...\\n\")\n",
    "print(f\"{'Runners':<10} {'Jobs':<10} {'Total (ms)':<12} {'Per Job (ms)':<15} {'Feasible %'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for num_runners in [5, 10, 50, 100]:\n",
    "    for num_jobs in [10, 100, 1000]:\n",
    "        result = benchmark_solver(num_runners, num_jobs)\n",
    "        print(f\"{result['runners']:<10} {result['jobs']:<10} \"\n",
    "              f\"{result['total_ms']:<12.2f} {result['per_job_ms']:<15.4f} \"\n",
    "              f\"{result['feasible_rate']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison: With vs Without Symbolic Filtering\n",
    "print(\"\\nğŸ“Š Symbolic Filtering Impact\\n\")\n",
    "print(\"Scenario: 100 runners, job requires [docker, gpu]\\n\")\n",
    "\n",
    "# Setup\n",
    "onto_large = RunnerOntology()\n",
    "for i in range(95):  # 95 regular runners\n",
    "    onto_large.add_runner(f\"runner-{i}\", capabilities=[\"docker\", \"linux\"])\n",
    "for i in range(5):   # 5 GPU runners\n",
    "    onto_large.add_runner(f\"gpu-runner-{i}\", capabilities=[\"docker\", \"linux\", \"gpu\"])\n",
    "\n",
    "solver_large = ConstraintSolver(onto_large, JobRequirementParser())\n",
    "result = solver_large.solve({\"tags\": [\"docker\"], \"image\": \"nvidia/cuda:11.8\"}, \"gpu-job\")\n",
    "\n",
    "print(f\"Total runners:     {len(onto_large.runners)}\")\n",
    "print(f\"Feasible runners:  {len(result.feasible_runners)}\")\n",
    "print(f\"Pruned runners:    {len(result.pruned_runners)}\")\n",
    "print(f\"\\nâ†’ MAB only needs to explore {len(result.feasible_runners)} options instead of {len(onto_large.runners)}!\")\n",
    "print(f\"â†’ Search space reduced by {(1 - len(result.feasible_runners)/len(onto_large.runners))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6ï¸âƒ£ Playground\n",
    "\n",
    "Experiment with your own runners and jobs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ® PLAYGROUND: Modify and experiment!\n",
    "\n",
    "# Create your own ontology\n",
    "my_onto = RunnerOntology()\n",
    "\n",
    "# Add your runners here:\n",
    "my_onto.add_runner(\"my-runner-1\", capabilities=[\"docker\", \"linux\"])\n",
    "my_onto.add_runner(\"my-runner-2\", capabilities=[\"shell\", \"macos\"])\n",
    "# Add more...\n",
    "\n",
    "# Create solver\n",
    "my_solver = ConstraintSolver(my_onto, JobRequirementParser())\n",
    "\n",
    "# Test a job\n",
    "my_job = {\n",
    "    \"tags\": [\"docker-any\"],\n",
    "    \"image\": \"python:3.11\"\n",
    "}\n",
    "\n",
    "result = my_solver.solve(my_job, \"my-test-job\")\n",
    "print(result.explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ® PLAYGROUND: Test with real .gitlab-ci.yml\n",
    "\n",
    "my_yaml = \"\"\"\n",
    "# Paste your .gitlab-ci.yml content here!\n",
    "\n",
    "test:\n",
    "  tags:\n",
    "    - docker-any\n",
    "  image: python:3.11\n",
    "  script:\n",
    "    - echo \"Hello\"\n",
    "\"\"\"\n",
    "\n",
    "my_jobs = parser.parse_yaml(my_yaml)\n",
    "for name, reqs in my_jobs.items():\n",
    "    print(f\"{name}: {reqs.to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7ï¸âƒ£ Production Ontology\n",
    "\n",
    "Load the actual blauweiss production configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load production ontology\n",
    "prod_onto = create_blauweiss_ontology()\n",
    "\n",
    "print(\"ğŸ­ Production Ontology:\\n\")\n",
    "for name, runner in prod_onto.runners.items():\n",
    "    status = \"ğŸŸ¢ online\" if runner.online else \"ğŸ”´ offline\"\n",
    "    caps = [c.name for c in runner.capabilities]\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Status: {status}\")\n",
    "    print(f\"  Capabilities: {caps}\")\n",
    "    print(f\"  Cost: â‚¬{runner.cost_per_minute}/min\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize for inspection\n",
    "print(\"ğŸ“„ Ontology as JSON:\\n\")\n",
    "print(prod_onto.to_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ Next Steps\n",
    "\n",
    "1. **Integration with MAB**: Connect to `runner_bandit` service\n",
    "2. **Real GitLab Webhooks**: Process actual job events\n",
    "3. **Paper Experiments**: More benchmarks, baselines\n",
    "4. **OWL Export**: Full semantic web compatibility\n",
    "\n",
    "---\n",
    "\n",
    "*NSAI - Neurosymbolic AI Runner Selection*  \n",
    "*Blauweiss LLC | JKU AI Bachelor 2026*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
