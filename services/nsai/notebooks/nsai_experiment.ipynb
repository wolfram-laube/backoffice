{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83e\uddea NSAI Experiment Notebook\n",
    "\n",
    "**Neurosymbolic Runner Selection \u2014 A/B Comparison & Analysis**\n",
    "\n",
    "| Version | Date | Author |\n",
    "|---------|------|--------|\n",
    "| 0.3.0 | 2026-02-06 | Wolfram Laube |\n",
    "\n",
    "This notebook runs reproducible experiments comparing three runner selection strategies:\n",
    "\n",
    "1. **Rule-Based** \u2014 always pick the first feasible runner (static baseline)\n",
    "2. **Pure MAB** \u2014 UCB1 over all runners, no constraint filtering\n",
    "3. **NSAI** \u2014 CSP filter \u2192 UCB1 (our neurosymbolic approach)\n",
    "\n",
    "Metrics: cumulative reward, regret, convergence speed, selection distribution.\n",
    "\n",
    "**Every section ends with `assert` cells \u2014 this notebook is a test suite.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import math, random, time\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "# Ensure nsai is importable (local, CI, or Colab)\n",
    "for p in [os.path.abspath(os.path.join(os.getcwd(), '..')),\n",
    "          os.path.abspath(os.path.join(os.getcwd(), '..', '..')),\n",
    "          '/content']:\n",
    "    if p not in sys.path:\n",
    "        sys.path.insert(0, p)\n",
    "\n",
    "from nsai import NeurosymbolicBandit, __version__\n",
    "from nsai.ontology import RunnerOntology, create_blauweiss_ontology\n",
    "from nsai.csp import ConstraintSolver, SolverStatus\n",
    "from nsai.parser import JobRequirementParser\n",
    "\n",
    "print(f'NSAI v{__version__}')\n",
    "print(f'Python {sys.version.split()[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 TestSuite: Setup \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "assert __version__ >= '0.3.0', f'Need NSAI >= 0.3.0, got {__version__}'\n",
    "\n",
    "onto = create_blauweiss_ontology()\n",
    "assert len(onto.runners) == 4, f'Expected 4 runners, got {len(onto.runners)}'\n",
    "\n",
    "expected_runners = {\n",
    "    'gitlab-runner-nordic', 'Mac Docker Runner',\n",
    "    'Mac2 Docker Runner', 'Linux Yoga Docker Runner'\n",
    "}\n",
    "assert set(onto.runners.keys()) == expected_runners, \\\n",
    "    f'Runner mismatch: {set(onto.runners.keys()) ^ expected_runners}'\n",
    "\n",
    "# MAB tag mapping roundtrip\n",
    "for name, runner in onto.runners.items():\n",
    "    assert runner.mab_tag, f'Runner {name} has no mab_tag'\n",
    "    assert onto.runner_name_for_mab_tag(runner.mab_tag) == name, \\\n",
    "        f'Tag roundtrip failed for {name}'\n",
    "\n",
    "print('\u2705 Setup: version, runners, tag mapping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Ground Truth Definition\n",
    "\n",
    "We simulate job execution with known per-runner success probabilities and durations.\n",
    "This lets us compute **optimal reward** and therefore **regret**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GROUND_TRUTH = {\n",
    "    'docker-any': {\n",
    "        'gitlab-runner-nordic':     {'p_success': 0.96, 'avg_duration': 18.0, 'cost': 0.01},\n",
    "        'Mac Docker Runner':        {'p_success': 0.92, 'avg_duration': 25.0, 'cost': 0.00},\n",
    "        'Mac2 Docker Runner':       {'p_success': 0.85, 'avg_duration': 35.0, 'cost': 0.00},\n",
    "        'Linux Yoga Docker Runner': {'p_success': 0.95, 'avg_duration': 15.0, 'cost': 0.00},\n",
    "    },\n",
    "    'gcp': {\n",
    "        'gitlab-runner-nordic':     {'p_success': 0.96, 'avg_duration': 18.0, 'cost': 0.01},\n",
    "        'Mac Docker Runner':        {'p_success': 0.00, 'avg_duration': 999,  'cost': 0.00},\n",
    "        'Mac2 Docker Runner':       {'p_success': 0.00, 'avg_duration': 999,  'cost': 0.00},\n",
    "        'Linux Yoga Docker Runner': {'p_success': 0.00, 'avg_duration': 999,  'cost': 0.00},\n",
    "    },\n",
    "    'shell': {\n",
    "        'gitlab-runner-nordic':     {'p_success': 0.94, 'avg_duration': 12.0, 'cost': 0.01},\n",
    "        'Mac Docker Runner':        {'p_success': 0.00, 'avg_duration': 999,  'cost': 0.00},\n",
    "        'Mac2 Docker Runner':       {'p_success': 0.00, 'avg_duration': 999,  'cost': 0.00},\n",
    "        'Linux Yoga Docker Runner': {'p_success': 0.93, 'avg_duration': 10.0, 'cost': 0.00},\n",
    "    },\n",
    "}\n",
    "\n",
    "def compute_reward(success: bool, duration: float, cost_per_min: float = 0.0) -> float:\n",
    "    \"\"\"Reward function matching NSAI/MAB: reward = success / (dur_min + cost_penalty + eps).\"\"\"\n",
    "    if not success:\n",
    "        return 0.0\n",
    "    dur_min = duration / 60.0\n",
    "    cost_penalty = cost_per_min * dur_min\n",
    "    return 1.0 / (dur_min + cost_penalty + 0.1)\n",
    "\n",
    "def simulate_job(runner: str, job_type: str, rng: random.Random) -> Tuple[bool, float]:\n",
    "    \"\"\"Simulate a job execution. Returns (success, duration_seconds).\"\"\"\n",
    "    profile = GROUND_TRUTH[job_type].get(runner, {'p_success': 0.0, 'avg_duration': 60.0})\n",
    "    success = rng.random() < profile['p_success']\n",
    "    duration = max(5.0, rng.gauss(profile['avg_duration'], profile['avg_duration'] * 0.2))\n",
    "    if not success:\n",
    "        duration = max(duration, rng.gauss(60.0, 10.0))\n",
    "    return success, duration\n",
    "\n",
    "# Show expected rewards\n",
    "print('Expected rewards per runner (docker-any):\\n')\n",
    "for runner, prof in GROUND_TRUTH['docker-any'].items():\n",
    "    e_reward = prof['p_success'] * compute_reward(True, prof['avg_duration'], prof['cost'])\n",
    "    print(f'  {runner:30} E[reward] = {e_reward:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 TestSuite: Ground Truth \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# All ground-truth runners must exist in ontology\n",
    "for job_type, runners in GROUND_TRUTH.items():\n",
    "    for runner in runners:\n",
    "        assert runner in onto.runners, \\\n",
    "            f'Ground truth runner \"{runner}\" not in ontology'\n",
    "\n",
    "# Reward function sanity\n",
    "assert compute_reward(True, 30.0) > 0, 'Success should yield positive reward'\n",
    "assert compute_reward(False, 30.0) == 0.0, 'Failure should yield zero reward'\n",
    "assert compute_reward(True, 10.0) > compute_reward(True, 60.0), \\\n",
    "    'Faster jobs should yield higher reward'\n",
    "assert compute_reward(True, 30.0, cost_per_min=0.0) > compute_reward(True, 30.0, cost_per_min=1.0), \\\n",
    "    'Higher cost should reduce reward'\n",
    "\n",
    "# Simulation determinism\n",
    "rng1 = random.Random(42)\n",
    "rng2 = random.Random(42)\n",
    "r1 = simulate_job('gitlab-runner-nordic', 'docker-any', rng1)\n",
    "r2 = simulate_job('gitlab-runner-nordic', 'docker-any', rng2)\n",
    "assert r1 == r2, 'Simulation must be deterministic with same seed'\n",
    "\n",
    "# Linux Yoga should have highest expected reward (docker-any)\n",
    "expected = {}\n",
    "for r, p in GROUND_TRUTH['docker-any'].items():\n",
    "    expected[r] = p['p_success'] * compute_reward(True, p['avg_duration'], p['cost'])\n",
    "best_runner = max(expected, key=expected.get)\n",
    "assert best_runner == 'Linux Yoga Docker Runner', \\\n",
    "    f'Expected Linux Yoga as optimal, got {best_runner}'\n",
    "\n",
    "print('\u2705 Ground truth: ontology alignment, reward monotonicity, determinism, optimal runner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Strategy Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuleBasedStrategy:\n",
    "    \"\"\"Always picks the first online runner (static baseline).\"\"\"\n",
    "    def __init__(self):\n",
    "        self.name = 'Rule-Based'\n",
    "        self._default = 'gitlab-runner-nordic'\n",
    "\n",
    "    def select(self, job_type: str) -> str:\n",
    "        return self._default\n",
    "\n",
    "    def update(self, runner, success, duration, cost=0.0):\n",
    "        pass\n",
    "\n",
    "\n",
    "class PureMABStrategy:\n",
    "    \"\"\"UCB1 over ALL runners \u2014 no symbolic filtering.\"\"\"\n",
    "    def __init__(self, runners: list, c: float = 2.0):\n",
    "        self.name = 'Pure MAB'\n",
    "        self.c = c\n",
    "        self._stats = {r: {'pulls': 0, 'total_reward': 0.0} for r in runners}\n",
    "        self._total = 0\n",
    "\n",
    "    def select(self, job_type: str) -> str:\n",
    "        for r, s in self._stats.items():\n",
    "            if s['pulls'] == 0:\n",
    "                return r\n",
    "        best, best_ucb = None, -1\n",
    "        for r, s in self._stats.items():\n",
    "            mean = s['total_reward'] / s['pulls']\n",
    "            explore = self.c * math.sqrt(math.log(self._total + 1) / s['pulls'])\n",
    "            ucb = mean + explore\n",
    "            if ucb > best_ucb:\n",
    "                best, best_ucb = r, ucb\n",
    "        return best\n",
    "\n",
    "    def update(self, runner, success, duration, cost=0.0):\n",
    "        reward = compute_reward(success, duration, cost)\n",
    "        self._stats[runner]['pulls'] += 1\n",
    "        self._stats[runner]['total_reward'] += reward\n",
    "        self._total += 1\n",
    "\n",
    "\n",
    "class NSAIStrategy:\n",
    "    \"\"\"Full NSAI: CSP filter \u2192 UCB1.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.name = 'NSAI'\n",
    "        self.nsai = NeurosymbolicBandit.create_default()\n",
    "\n",
    "    def select(self, job_type: str) -> str:\n",
    "        tags = [job_type] if job_type != 'docker-any' else ['docker-any']\n",
    "        runner, _ = self.nsai.select_runner({'tags': tags})\n",
    "        return runner or 'gitlab-runner-nordic'\n",
    "\n",
    "    def update(self, runner, success, duration, cost=0.0):\n",
    "        self.nsai.update(runner, success, duration, cost)\n",
    "\n",
    "print('Strategies defined \u2713')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 TestSuite: Strategy Sanity \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "runners_list = list(GROUND_TRUTH['docker-any'].keys())\n",
    "\n",
    "# Rule-Based always returns same runner\n",
    "rb = RuleBasedStrategy()\n",
    "assert rb.select('docker-any') == rb.select('gcp') == 'gitlab-runner-nordic'\n",
    "\n",
    "# Pure MAB explores first, then exploits (must update between selects)\n",
    "mab = PureMABStrategy(runners_list)\n",
    "first_4 = []\n",
    "for _ in range(4):\n",
    "    sel_m = mab.select('docker-any')\n",
    "    first_4.append(sel_m)\n",
    "    mab.update(sel_m, success=True, duration=20.0)\n",
    "assert len(set(first_4)) == 4, \\\n",
    "    f'MAB should explore all 4 runners first, visited {len(set(first_4))}'\n",
    "\n",
    "# NSAI returns a valid runner\n",
    "ns = NSAIStrategy()\n",
    "sel = ns.select('docker-any')\n",
    "assert sel in runners_list, f'NSAI selected unknown runner: {sel}'\n",
    "\n",
    "print('\u2705 Strategies: rule-based static, MAB explores all, NSAI valid selection')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Run Experiment (docker-any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(n_rounds=200, seed=42, job_type='docker-any'):\n",
    "    \"\"\"Run A/B experiment and return per-round results.\"\"\"\n",
    "    rng = random.Random(seed)\n",
    "    runners = list(GROUND_TRUTH[job_type].keys())\n",
    "\n",
    "    strategies = [\n",
    "        RuleBasedStrategy(),\n",
    "        PureMABStrategy(runners),\n",
    "        NSAIStrategy(),\n",
    "    ]\n",
    "\n",
    "    # Optimal expected reward for regret calculation\n",
    "    best_expected = max(\n",
    "        p['p_success'] * compute_reward(True, p['avg_duration'], p['cost'])\n",
    "        for p in GROUND_TRUTH[job_type].values()\n",
    "        if p['p_success'] > 0\n",
    "    )\n",
    "\n",
    "    results = {s.name: {\n",
    "        'rewards': [], 'cum_reward': [], 'regret': [],\n",
    "        'cum_regret': [], 'selections': []\n",
    "    } for s in strategies}\n",
    "\n",
    "    for t in range(n_rounds):\n",
    "        for strategy in strategies:\n",
    "            r = results[strategy.name]\n",
    "            runner = strategy.select(job_type)\n",
    "            success, duration = simulate_job(runner, job_type, rng)\n",
    "            cost = GROUND_TRUTH[job_type].get(runner, {}).get('cost', 0.0)\n",
    "            reward = compute_reward(success, duration, cost)\n",
    "            strategy.update(runner, success, duration, cost)\n",
    "            r['rewards'].append(reward)\n",
    "            r['cum_reward'].append(sum(r['rewards']))\n",
    "            r['regret'].append(best_expected - reward)\n",
    "            r['cum_regret'].append(sum(r['regret']))\n",
    "            r['selections'].append(runner)\n",
    "\n",
    "    return results, best_expected\n",
    "\n",
    "N_ROUNDS = 300\n",
    "results, optimal_reward = run_experiment(n_rounds=N_ROUNDS, seed=42)\n",
    "print(f'Experiment complete: {N_ROUNDS} rounds \u00d7 {len(results)} strategies')\n",
    "print(f'Optimal expected reward per round: {optimal_reward:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 TestSuite: Experiment Integrity \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "for name, r in results.items():\n",
    "    assert len(r['rewards']) == N_ROUNDS, \\\n",
    "        f'{name}: expected {N_ROUNDS} rounds, got {len(r[\"rewards\"])}'\n",
    "    assert len(r['selections']) == N_ROUNDS\n",
    "    assert len(r['cum_regret']) == N_ROUNDS\n",
    "\n",
    "    # Cumulative reward must be monotonically non-decreasing\n",
    "    for i in range(1, len(r['cum_reward'])):\n",
    "        assert r['cum_reward'][i] >= r['cum_reward'][i-1], \\\n",
    "            f'{name}: cum_reward decreased at round {i}'\n",
    "\n",
    "    # All rewards must be non-negative\n",
    "    assert all(rw >= 0 for rw in r['rewards']), f'{name}: negative reward found'\n",
    "\n",
    "    # All selections must be valid runners\n",
    "    valid = set(GROUND_TRUTH['docker-any'].keys())\n",
    "    invalid = set(r['selections']) - valid\n",
    "    assert not invalid, f'{name}: invalid selections {invalid}'\n",
    "\n",
    "# Determinism: same seed \u2192 same results\n",
    "results2, _ = run_experiment(n_rounds=50, seed=42)\n",
    "for name in results:\n",
    "    assert results[name]['rewards'][:50] == results2[name]['rewards'], \\\n",
    "        f'{name}: not deterministic with same seed'\n",
    "\n",
    "print(f'\u2705 Experiment integrity: {N_ROUNDS} rounds, monotonic rewards, valid selections, deterministic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Results: Cumulative Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('=== Cumulative Reward (after %d rounds) ===\\n' % N_ROUNDS)\n",
    "for name, r in sorted(results.items(), key=lambda x: -x[1]['cum_reward'][-1]):\n",
    "    total = r['cum_reward'][-1]\n",
    "    avg = total / N_ROUNDS\n",
    "    bar = '\u2588' * int(total / 5)\n",
    "    print(f'{name:15} {total:7.1f}  (avg {avg:.3f})  {bar}')\n",
    "\n",
    "print('\\n=== Cumulative Regret (lower is better) ===\\n')\n",
    "for name, r in sorted(results.items(), key=lambda x: x[1]['cum_regret'][-1]):\n",
    "    total = r['cum_regret'][-1]\n",
    "    bar = '\u2591' * int(total / 2)\n",
    "    print(f'{name:15} {total:7.1f}  {bar}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 TestSuite: Reward Comparison \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "nsai_total  = results['NSAI']['cum_reward'][-1]\n",
    "rule_total  = results['Rule-Based']['cum_reward'][-1]\n",
    "mab_total   = results['Pure MAB']['cum_reward'][-1]\n",
    "\n",
    "# NSAI must beat Rule-Based (the whole point)\n",
    "assert nsai_total > rule_total, \\\n",
    "    f'NSAI ({nsai_total:.1f}) should beat Rule-Based ({rule_total:.1f})'\n",
    "\n",
    "# All strategies must have positive total reward\n",
    "for name, r in results.items():\n",
    "    assert r['cum_reward'][-1] > 0, f'{name} has zero total reward'\n",
    "\n",
    "# NSAI should achieve at least 80% of Pure MAB reward\n",
    "# (CSP adds overhead on unconstrained jobs, but shouldn't destroy performance)\n",
    "ratio = nsai_total / mab_total\n",
    "assert ratio > 0.80, \\\n",
    "    f'NSAI/MAB ratio {ratio:.2f} too low (threshold 0.80)'\n",
    "\n",
    "# NSAI regret must be finite and less than Rule-Based regret\n",
    "nsai_regret = results['NSAI']['cum_regret'][-1]\n",
    "rule_regret = results['Rule-Based']['cum_regret'][-1]\n",
    "assert nsai_regret < rule_regret, \\\n",
    "    f'NSAI regret ({nsai_regret:.1f}) should be < Rule-Based ({rule_regret:.1f})'\n",
    "\n",
    "print(f'\u2705 Reward: NSAI ({nsai_total:.0f}) > Rule-Based ({rule_total:.0f}), '\n",
    "      f'NSAI/MAB ratio = {ratio:.2f}, regret NSAI ({nsai_regret:.0f}) < Rule ({rule_regret:.0f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Results: Selection Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNNERS_SHORT = {\n",
    "    'gitlab-runner-nordic': 'nordic',\n",
    "    'Mac Docker Runner': 'mac',\n",
    "    'Mac2 Docker Runner': 'mac2',\n",
    "    'Linux Yoga Docker Runner': 'linux',\n",
    "}\n",
    "\n",
    "print('=== Runner Selection Distribution ===\\n')\n",
    "for name, r in results.items():\n",
    "    counts = Counter(r['selections'])\n",
    "    total = len(r['selections'])\n",
    "    print(f'--- {name} ---')\n",
    "    for runner in GROUND_TRUTH['docker-any']:\n",
    "        n = counts.get(runner, 0)\n",
    "        pct = n / total * 100\n",
    "        bar = '\u2593' * int(pct / 2)\n",
    "        print(f'  {RUNNERS_SHORT[runner]:8} {n:4}/{total}  ({pct:5.1f}%)  {bar}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 TestSuite: Selection Distribution \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Rule-Based: must only select one runner\n",
    "rb_unique = set(results['Rule-Based']['selections'])\n",
    "assert len(rb_unique) == 1, f'Rule-Based selected {len(rb_unique)} runners (expected 1)'\n",
    "\n",
    "# Pure MAB: must have explored all 4 runners\n",
    "mab_unique = set(results['Pure MAB']['selections'])\n",
    "assert len(mab_unique) == 4, f'Pure MAB only explored {len(mab_unique)} runners'\n",
    "\n",
    "# NSAI: must have explored all 4 runners (all are feasible for docker-any)\n",
    "nsai_unique = set(results['NSAI']['selections'])\n",
    "assert len(nsai_unique) == 4, f'NSAI only explored {len(nsai_unique)} runners'\n",
    "\n",
    "# Both learning strategies should favor Linux Yoga (optimal) in majority\n",
    "for name in ['Pure MAB', 'NSAI']:\n",
    "    counts = Counter(results[name]['selections'])\n",
    "    top_runner = counts.most_common(1)[0][0]\n",
    "    top_pct = counts[top_runner] / N_ROUNDS\n",
    "    assert top_runner == 'Linux Yoga Docker Runner', \\\n",
    "        f'{name} top runner is {RUNNERS_SHORT[top_runner]}, expected linux'\n",
    "    assert top_pct > 0.50, \\\n",
    "        f'{name} selected optimal runner only {top_pct:.0%} (need >50%)'\n",
    "\n",
    "# Mac2 (worst) should be selected least by learning strategies\n",
    "for name in ['Pure MAB', 'NSAI']:\n",
    "    counts = Counter(results[name]['selections'])\n",
    "    mac2_count = counts.get('Mac2 Docker Runner', 0)\n",
    "    linux_count = counts.get('Linux Yoga Docker Runner', 0)\n",
    "    assert mac2_count < linux_count, \\\n",
    "        f'{name}: mac2 ({mac2_count}) should be selected less than linux ({linux_count})'\n",
    "\n",
    "print('\u2705 Distribution: Rule-Based static, both learners favor linux, avoid mac2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Convergence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convergence_round(selections: list, optimal: str,\n",
    "                       window: int = 20, threshold: float = 0.7) -> int:\n",
    "    \"\"\"Find the round where strategy converges to selecting optimal runner.\n",
    "    Returns -1 if not converged.\"\"\"\n",
    "    for i in range(window, len(selections)):\n",
    "        recent = selections[i-window:i]\n",
    "        if recent.count(optimal) / window >= threshold:\n",
    "            return i - window\n",
    "    return -1\n",
    "\n",
    "optimal = 'Linux Yoga Docker Runner'\n",
    "\n",
    "print(f'=== Convergence to Optimal Runner ({RUNNERS_SHORT[optimal]}) ===\\n')\n",
    "print(f'Criterion: \u226570% selection rate in rolling window of 20\\n')\n",
    "\n",
    "convergence = {}\n",
    "for name, r in results.items():\n",
    "    conv = convergence_round(r['selections'], optimal)\n",
    "    convergence[name] = conv\n",
    "    if conv >= 0:\n",
    "        print(f'{name:15} converged at round {conv}')\n",
    "    else:\n",
    "        pct = Counter(r['selections']).get(optimal, 0) / N_ROUNDS * 100\n",
    "        print(f'{name:15} did not converge ({pct:.0f}% overall)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 TestSuite: Convergence \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Rule-Based should NOT converge (never selects Linux Yoga)\n",
    "assert convergence['Rule-Based'] == -1, \\\n",
    "    'Rule-Based should not converge to optimal'\n",
    "\n",
    "# Both learning strategies should converge within 200 rounds\n",
    "for name in ['Pure MAB', 'NSAI']:\n",
    "    assert convergence[name] >= 0, f'{name} did not converge'\n",
    "    assert convergence[name] < 200, \\\n",
    "        f'{name} converged too late (round {convergence[name]})'\n",
    "\n",
    "# Regret in second half should be less than first half (learning effect)\n",
    "half = N_ROUNDS // 2\n",
    "for name in ['Pure MAB', 'NSAI']:\n",
    "    r = results[name]\n",
    "    regret_1st = sum(r['regret'][:half])\n",
    "    regret_2nd = sum(r['regret'][half:])\n",
    "    assert regret_2nd <= regret_1st * 1.1, \\\n",
    "        f'{name}: regret not decreasing (1st={regret_1st:.1f}, 2nd={regret_2nd:.1f})'\n",
    "\n",
    "print(f'\u2705 Convergence: MAB@{convergence[\"Pure MAB\"]}, NSAI@{convergence[\"NSAI\"]}, '\n",
    "      f'Rule-Based never, regret sublinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Regret Curve (ASCII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascii_plot(series_dict: dict, title: str, width: int = 60, height: int = 18):\n",
    "    \"\"\"Simple ASCII line chart.\"\"\"\n",
    "    all_vals = [v for vals in series_dict.values() for v in vals]\n",
    "    y_min, y_max = min(all_vals), max(all_vals)\n",
    "    if y_max == y_min:\n",
    "        y_max = y_min + 1\n",
    "    x_len = max(len(v) for v in series_dict.values())\n",
    "    symbols = {'Rule-Based': '.', 'Pure MAB': '+', 'NSAI': '*'}\n",
    "    grid = [[' '] * width for _ in range(height)]\n",
    "\n",
    "    for name, vals in series_dict.items():\n",
    "        sym = symbols.get(name, '?')\n",
    "        step = max(1, len(vals) // width)\n",
    "        for xi in range(0, min(len(vals), width * step), step):\n",
    "            col = xi // step\n",
    "            if col >= width:\n",
    "                break\n",
    "            row = int((vals[xi] - y_min) / (y_max - y_min) * (height - 1))\n",
    "            row = height - 1 - row\n",
    "            grid[row][col] = sym\n",
    "\n",
    "    print(f'\\n{title}')\n",
    "    print('\u2500' * (width + 10))\n",
    "    for i, row in enumerate(grid):\n",
    "        y_val = y_max - (y_max - y_min) * i / (height - 1)\n",
    "        print(f'{y_val:7.1f} \u2502{\"\".join(row)}')\n",
    "    print(f'        \u2514{\"\u2500\" * width}')\n",
    "    print(f'         0{\" \" * (width - 10)}round {x_len}')\n",
    "    print(f'  Legend: ', end='')\n",
    "    for name, sym in symbols.items():\n",
    "        print(f'{sym}={name}  ', end='')\n",
    "    print()\n",
    "\n",
    "ascii_plot(\n",
    "    {name: r['cum_regret'] for name, r in results.items()},\n",
    "    'Cumulative Regret (lower is better)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Constraint-Intensive Experiment (GCP-only)\n",
    "\n",
    "This is where NSAI shines: only 1 runner is feasible for GCP jobs.\n",
    "Pure MAB wastes rounds trying infeasible runners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gcp, _ = run_experiment(n_rounds=100, seed=42, job_type='gcp')\n",
    "\n",
    "print('=== GCP-Only Experiment (100 rounds) ===\\n')\n",
    "for name, r in sorted(results_gcp.items(), key=lambda x: -x[1]['cum_reward'][-1]):\n",
    "    total = r['cum_reward'][-1]\n",
    "    unique = len(set(r['selections']))\n",
    "    fails = sum(1 for rw in r['rewards'] if rw == 0)\n",
    "    print(f'{name:15} reward={total:6.1f}  runners_tried={unique}  failures={fails}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 TestSuite: Constraint-Intensive (GCP) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "nsai_gcp   = results_gcp['NSAI']['cum_reward'][-1]\n",
    "mab_gcp    = results_gcp['Pure MAB']['cum_reward'][-1]\n",
    "rule_gcp   = results_gcp['Rule-Based']['cum_reward'][-1]\n",
    "\n",
    "# NSAI should match or beat Pure MAB on constrained jobs\n",
    "# because CSP immediately filters to only nordic\n",
    "assert nsai_gcp >= mab_gcp * 0.95, \\\n",
    "    f'NSAI ({nsai_gcp:.1f}) should match MAB ({mab_gcp:.1f}) on GCP jobs'\n",
    "\n",
    "# NSAI should only select nordic (the only GCP runner)\n",
    "nsai_gcp_selections = set(results_gcp['NSAI']['selections'])\n",
    "assert nsai_gcp_selections == {'gitlab-runner-nordic'}, \\\n",
    "    f'NSAI selected non-GCP runners: {nsai_gcp_selections}'\n",
    "\n",
    "# Pure MAB will waste rounds on infeasible runners\n",
    "mab_gcp_unique = set(results_gcp['Pure MAB']['selections'])\n",
    "mab_gcp_failures = sum(1 for rw in results_gcp['Pure MAB']['rewards'] if rw == 0)\n",
    "\n",
    "nsai_gcp_failures = sum(1 for rw in results_gcp['NSAI']['rewards'] if rw == 0)\n",
    "\n",
    "# NSAI should have fewer failures (no wasted rounds on infeasible runners)\n",
    "# Only natural failures (4% of nordic jobs)\n",
    "assert nsai_gcp_failures <= mab_gcp_failures, \\\n",
    "    f'NSAI failures ({nsai_gcp_failures}) should be <= MAB failures ({mab_gcp_failures})'\n",
    "\n",
    "print(f'\u2705 GCP constraint test: NSAI only selected nordic, '\n",
    "      f'failures NSAI={nsai_gcp_failures} <= MAB={mab_gcp_failures}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Live MAB Service Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "MAB_URL = 'https://runner-bandit-m5cziijwqa-lz.a.run.app'\n",
    "live_available = False\n",
    "\n",
    "try:\n",
    "    with urllib.request.urlopen(f'{MAB_URL}/stats', timeout=5) as resp:\n",
    "        live_stats = json.loads(resp.read())\n",
    "    live_available = True\n",
    "\n",
    "    print(f'MAB Service: {live_stats[\"algorithm\"]}')\n",
    "    print(f'Total observations: {live_stats[\"total_observations\"]}\\n')\n",
    "\n",
    "    for runner, stats in live_stats.get('runners', {}).items():\n",
    "        print(f'{runner:30} pulls={stats[\"pulls\"]:3}  '\n",
    "              f'success={stats[\"success_rate\"]:.0%}  '\n",
    "              f'avg_dur={stats[\"avg_duration\"]:.1f}s  '\n",
    "              f'reward={stats[\"mean_reward\"]:.3f}')\n",
    "\n",
    "    nsai_live = NeurosymbolicBandit.from_live_service(MAB_URL)\n",
    "    print(f'\\n\u2705 NSAI warm-started from live service')\n",
    "    print(f'   Total pulls synced: {nsai_live._total_pulls}')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'\u26a0\ufe0f  MAB service not reachable: {e}')\n",
    "    print('Continuing with cold-start NSAI (live tests skipped).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 TestSuite: Live MAB Service \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "if live_available:\n",
    "    # Service should report correct algorithm\n",
    "    assert 'UCB1' in live_stats['algorithm'], \\\n",
    "        f'Unexpected algorithm: {live_stats[\"algorithm\"]}'\n",
    "\n",
    "    # All 4 runners must be registered\n",
    "    live_runners = set(live_stats['runners'].keys())\n",
    "    assert live_runners == expected_runners, \\\n",
    "        f'Runner mismatch: expected {expected_runners}, got {live_runners}'\n",
    "\n",
    "    # Stats must have valid ranges\n",
    "    for runner, stats in live_stats['runners'].items():\n",
    "        assert 0 <= stats['success_rate'] <= 1.0, \\\n",
    "            f'{runner}: invalid success_rate {stats[\"success_rate\"]}'\n",
    "        assert stats['pulls'] >= 0, \\\n",
    "            f'{runner}: negative pulls {stats[\"pulls\"]}'\n",
    "        assert stats['avg_duration'] >= 0, \\\n",
    "            f'{runner}: negative duration'\n",
    "\n",
    "    # Warm-started NSAI should have synced total pulls\n",
    "    assert nsai_live._total_pulls == live_stats['total_observations'], \\\n",
    "        f'Sync mismatch: {nsai_live._total_pulls} vs {live_stats[\"total_observations\"]}'\n",
    "\n",
    "    # Warm-started NSAI should still be able to select runners\n",
    "    runner, exp = nsai_live.select_runner({'tags': ['docker-any']})\n",
    "    assert runner in expected_runners\n",
    "\n",
    "    print(f'\u2705 Live MAB: algorithm OK, {len(live_runners)} runners, '\n",
    "          f'{live_stats[\"total_observations\"]} obs synced, selection works')\n",
    "else:\n",
    "    print('\u23ed\ufe0f  Live tests skipped (service offline)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. NSAI Explanation Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsai_demo = NeurosymbolicBandit.create_default()\n",
    "rng = random.Random(42)\n",
    "\n",
    "# Quick training\n",
    "for _ in range(30):\n",
    "    runner, _ = nsai_demo.select_runner({'tags': ['docker-any']})\n",
    "    success, dur = simulate_job(runner, 'docker-any', rng)\n",
    "    nsai_demo.update(runner, success, dur)\n",
    "\n",
    "print('=== Docker Job ===')\n",
    "runner_d, exp_d = nsai_demo.select_runner({'tags': ['docker-any']}, job_name='test:unit')\n",
    "print(exp_d)\n",
    "\n",
    "print('\\n=== GCP-Only Job ===')\n",
    "runner_g, exp_g = nsai_demo.select_runner({'tags': ['docker-any', 'gcp']}, job_name='cloud-run:build')\n",
    "print(exp_g)\n",
    "\n",
    "print('\\n=== Impossible Job ===')\n",
    "runner_x, exp_x = nsai_demo.select_runner({'tags': ['gpu', 'arm64']}, job_name='ml:train')\n",
    "print(exp_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 TestSuite: Explanation Quality \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# Docker job: feasible, multiple runners\n",
    "assert runner_d is not None, 'Docker job should find a runner'\n",
    "assert len(exp_d.feasible_runners) == 4, \\\n",
    "    f'Docker job: expected 4 feasible, got {len(exp_d.feasible_runners)}'\n",
    "assert exp_d.confidence > 0, 'Confidence should be positive'\n",
    "assert exp_d.solve_time_ms > 0, 'Solve time should be measured'\n",
    "assert len(exp_d.symbolic_reasoning) > 10, 'Symbolic reasoning too short'\n",
    "assert len(exp_d.statistical_reasoning) > 10, 'Statistical reasoning too short'\n",
    "\n",
    "# GCP job: only nordic feasible\n",
    "assert runner_g == 'gitlab-runner-nordic', \\\n",
    "    f'GCP job should select nordic, got {runner_g}'\n",
    "assert exp_g.feasible_runners == ['gitlab-runner-nordic'], \\\n",
    "    f'GCP: expected only nordic feasible, got {exp_g.feasible_runners}'\n",
    "\n",
    "# Impossible job: no feasible runner\n",
    "assert runner_x is None, 'Impossible job should return None'\n",
    "assert exp_x.confidence == 0.0, 'Impossible job confidence should be 0'\n",
    "assert len(exp_x.feasible_runners) == 0, 'Impossible job should have 0 feasible'\n",
    "assert 'No feasible' in exp_x.statistical_reasoning\n",
    "\n",
    "# Explanation serialization roundtrip\n",
    "d = exp_d.to_dict()\n",
    "assert d['selected_runner'] == runner_d\n",
    "assert isinstance(d['feasible_runners'], list)\n",
    "assert isinstance(d['confidence'], float)\n",
    "\n",
    "print('\u2705 Explanations: docker (4 feasible), GCP (nordic only), impossible (None), serializable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Performance Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selection latency benchmark\n",
    "nsai_bench = NeurosymbolicBandit.create_default()\n",
    "# Warm up with some data\n",
    "for r in nsai_bench._stats:\n",
    "    nsai_bench.update(r, success=True, duration_seconds=20.0)\n",
    "\n",
    "n_iters = 1000\n",
    "start = time.perf_counter()\n",
    "for _ in range(n_iters):\n",
    "    nsai_bench.select_runner({'tags': ['docker-any']})\n",
    "elapsed = (time.perf_counter() - start) * 1000\n",
    "avg_ms = elapsed / n_iters\n",
    "\n",
    "print(f'Selection latency: {avg_ms:.3f}ms avg ({n_iters} iterations)')\n",
    "print(f'Throughput: {n_iters / (elapsed/1000):.0f} selections/sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 TestSuite: Performance \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "assert avg_ms < 5.0, f'Selection too slow: {avg_ms:.3f}ms (limit: 5ms)'\n",
    "\n",
    "# Update latency\n",
    "start = time.perf_counter()\n",
    "for _ in range(1000):\n",
    "    nsai_bench.update('gitlab-runner-nordic', True, 20.0)\n",
    "update_ms = (time.perf_counter() - start) * 1000 / 1000\n",
    "assert update_ms < 1.0, f'Update too slow: {update_ms:.3f}ms (limit: 1ms)'\n",
    "\n",
    "# Memory: stats dict shouldn't grow unbounded\n",
    "assert len(nsai_bench._stats) == 4, 'Stats should only track registered runners'\n",
    "\n",
    "print(f'\u2705 Performance: select={avg_ms:.3f}ms, update={update_ms:.3f}ms, memory=4 runners')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Summary & Final Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557')\n",
    "print('\u2551              NSAI Experiment Summary                        \u2551')\n",
    "print('\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563')\n",
    "print(f'\u2551  NSAI Version:         {__version__:>8}                            \u2551')\n",
    "print(f'\u2551  Runners:              {len(onto.runners):>8}                            \u2551')\n",
    "print(f'\u2551  Experiment Rounds:    {N_ROUNDS:>8}                            \u2551')\n",
    "print('\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563')\n",
    "for name in ['NSAI', 'Pure MAB', 'Rule-Based']:\n",
    "    r = results[name]\n",
    "    cr = r['cum_reward'][-1]\n",
    "    reg = r['cum_regret'][-1]\n",
    "    print(f'\u2551  {name:15}  reward={cr:7.1f}  regret={reg:7.1f}          \u2551')\n",
    "print('\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563')\n",
    "print(f'\u2551  NSAI vs Rule-Based:  {((nsai_total/rule_total - 1)*100):+5.1f}% reward                  \u2551')\n",
    "print(f'\u2551  NSAI vs Pure MAB:    {((nsai_total/mab_total - 1)*100):+5.1f}% reward                  \u2551')\n",
    "print(f'\u2551  GCP advantage:        NSAI 0 wasted rounds              \u2551')\n",
    "if live_available:\n",
    "    print(f'\u2551  Live MAB:             {live_stats[\"total_observations\"]:>4} observations synced        \u2551')\n",
    "print('\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 TestSuite: Final Gate \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
    "# This cell summarizes ALL assertions \u2014 if we get here, everything passed.\n",
    "\n",
    "PASSED = []\n",
    "\n",
    "# Re-validate core invariants\n",
    "assert __version__ >= '0.3.0';                        PASSED.append('version')\n",
    "assert len(onto.runners) == 4;                        PASSED.append('runner_count')\n",
    "assert nsai_total > rule_total;                       PASSED.append('nsai_beats_rule')\n",
    "assert nsai_total > mab_total * 0.80;                 PASSED.append('nsai_within_20pct_mab')\n",
    "assert nsai_regret < rule_regret;                     PASSED.append('nsai_lower_regret')\n",
    "assert convergence['NSAI'] >= 0;                      PASSED.append('nsai_converges')\n",
    "assert convergence['NSAI'] < 200;                     PASSED.append('nsai_converges_fast')\n",
    "assert nsai_gcp_failures <= mab_gcp_failures;         PASSED.append('gcp_fewer_failures')\n",
    "assert avg_ms < 5.0;                                  PASSED.append('latency_ok')\n",
    "assert runner_x is None;                              PASSED.append('infeasible_handled')\n",
    "\n",
    "print(f'\\n\ud83c\udfc1 ALL {len(PASSED)} ASSERTIONS PASSED')\n",
    "print(f'   {\", \".join(PASSED)}')\n",
    "print(f'\\n   Notebook is a valid test suite. \u2705')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}